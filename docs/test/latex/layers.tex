Detailed information for layer types and options within these types can be found in this section. The layers are \hyperlink{layers_learning}{Learning}, \hyperlink{layers_regularizer}{Regularizer}, \hyperlink{layers_transform}{Transform}, \hyperlink{layers_activation}{Activation}, and \hyperlink{layers_i_o}{IO}.\hypertarget{layers_learning}{}\section{Learning}\label{layers_learning}
\hypertarget{layers_conv}{}\subsection{Convolution}\label{layers_conv}
Implementation details\+: \hyperlink{classlbann_1_1convolution__layer}{lbann\+::convolution\+\_\+layer}\hypertarget{layers_deconv}{}\subsection{Deconvolution}\label{layers_deconv}
Implementation details\+: \hyperlink{classlbann_1_1deconvolution__layer}{lbann\+::deconvolution\+\_\+layer}\hypertarget{layers_ip}{}\subsection{Fully Connected}\label{layers_ip}
Fully-\/connected layer. This layer applies an affine transformation.

Implementation details\+: \hyperlink{classlbann_1_1fully__connected__layer}{lbann\+::fully\+\_\+connected\+\_\+layer}\hypertarget{layers_regularizer}{}\section{Regularizer}\label{layers_regularizer}
\hypertarget{layers_batchNorm}{}\subsection{Batch Normalization}\label{layers_batchNorm}
Batch normalization layer. Each input channel is normalized across the mini-\/batch to have zero mean and unit standard deviation. Learned scaling factors and biases are then applied. See\+: Sergey Ioffe and Christian Szegedy. "Batch Normalization\+: Accelerating Deep Network Training by Reducing Internal Covariate Shift." I\+C\+ML 2015. This uses the standard approach of maintaining the running mean and standard deviation (with exponential decay) for use at test time. See\+: \href{https://cthorey.github.io/backpropagation/}{\tt https\+://cthorey.\+github.\+io/backpropagation/}

Implementation details\+: \hyperlink{classlbann_1_1batch__normalization}{lbann\+::batch\+\_\+normalization}\hypertarget{layers_dropout}{}\subsection{Dropout}\label{layers_dropout}
Dropout layer\+: Probabilistically drop layer outputs. See\+: Srivastava, Nitish, et al. \char`\"{}\+Dropout\+: a simple way to prevent
  neural networks from overfitting.\char`\"{} Journal of Machine Learning Research 15.\+1 (2014). The weights are multiplied by 1/(keep probability) at training time, as discussed in section 10 of the paper. Keep probabilities of 0.\+5 for fully-\/connected layers and 0.\+8 for input layers are good starting points.

Implementation details\+: \hyperlink{classlbann_1_1dropout}{lbann\+::dropout}\hypertarget{layers_selu_dropout}{}\subsection{Selu Dropout}\label{layers_selu_dropout}
S\+E\+LU dropout\+: alpha-\/scaled dropout for use with S\+E\+LU activations. See\+: Klambauer et al. \char`\"{}\+Self-\/\+Normalizing Neural Networks\char`\"{}, 2017. This makes the same default assumptions as our S\+E\+LU activations. The paper recommends a default dropout rate of 0.\+05 (keep 0.\+95).

Implementation details\+: \hyperlink{classlbann_1_1selu__dropout}{lbann\+::selu\+\_\+dropout}\hypertarget{layers_local_response_norm_layer}{}\subsection{Local Response Norm Layer}\label{layers_local_response_norm_layer}
Local Response Normalization layer.

Implementation details\+: \hyperlink{classlbann_1_1local__response__normalization__layer}{lbann\+::local\+\_\+response\+\_\+normalization\+\_\+layer}\hypertarget{layers_transform}{}\section{Transform}\label{layers_transform}
\hypertarget{layers_concatenation}{}\subsection{Concatenation}\label{layers_concatenation}
Concatenation layer. This layer concatenates input tensors along a specified axis.

Implementation details\+: \hyperlink{classlbann_1_1concatenation__layer}{lbann\+::concatenation\+\_\+layer}\hypertarget{layers_noise}{}\subsection{Noise}\label{layers_noise}
Implementation details\+: lbann\+::noise\+\_\+layer\hypertarget{layers_unpooling}{}\subsection{Unpooling}\label{layers_unpooling}
Unpooling layer.

Implementation details\+: \hyperlink{classlbann_1_1unpooling__layer}{lbann\+::unpooling\+\_\+layer}\hypertarget{layers_pooling}{}\subsection{Pooling}\label{layers_pooling}
Pooling layer.

Implementation details\+: \hyperlink{classlbann_1_1pooling__layer}{lbann\+::pooling\+\_\+layer}\hypertarget{layers_reshape}{}\subsection{Reshape}\label{layers_reshape}
Reshape layer

Implementation details\+: \hyperlink{classlbann_1_1pooling__layer}{lbann\+::pooling\+\_\+layer}\hypertarget{layers_slice}{}\subsection{Slice}\label{layers_slice}
Slice layer. This layer slices an input tensor along a specified axis.

Implementation details\+: \hyperlink{classlbann_1_1slice__layer}{lbann\+::slice\+\_\+layer}\hypertarget{layers_split}{}\subsection{Split}\label{layers_split}
Split layer. This layer can accommodate an arbitrary number of outputs.

Implementation details\+: \hyperlink{classlbann_1_1split__layer}{lbann\+::split\+\_\+layer}\hypertarget{layers_sum}{}\subsection{Sum}\label{layers_sum}
Sum layer. This layer performs a weighted sum of input tensors, possibly with a different scaling factor for each input. If the scaling factors are not provided, they are all set to one so that this layer performs a simple sum.

Implementation details\+: \hyperlink{classlbann_1_1sum__layer}{lbann\+::sum\+\_\+layer}\hypertarget{layers_activation}{}\section{Activation}\label{layers_activation}
\hypertarget{layers_idlayer}{}\subsection{Identity}\label{layers_idlayer}
Implementation details\+: lbann\+::id\+\_\+layer\hypertarget{layers_reluLayer}{}\subsection{Rectified Linear Unit}\label{layers_reluLayer}
Rectified linear unit activation function. See \href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{\tt https\+://en.\+wikipedia.\+org/wiki/\+Rectifier\+\_\+(neural\+\_\+networks)}

Implementation details\+: \hyperlink{classlbann_1_1relu__layer}{lbann\+::relu\+\_\+layer}\hypertarget{layers_leakyrelu}{}\subsection{Leaky Relu}\label{layers_leakyrelu}
Leaky rectified linear unit activation function. This is a Re\+LU variant that avoids the dying Re\+LU problem where a Re\+LU neuron can stop updating. See\+: Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. \char`\"{}\+Rectifier
nonlinearities improve neural network acoustic models.\char`\"{} Proc. I\+C\+ML. Vol. 30. No. 1. 2013.

Implementation details\+: \hyperlink{classlbann_1_1leaky__relu__layer}{lbann\+::leaky\+\_\+relu\+\_\+layer}\+:\hypertarget{layers_smoothrelu}{}\subsection{Smooth Relu}\label{layers_smoothrelu}
Smooth Rectified linear unit activation function. This is an approximation to the softplus.

Implementation details\+: \hyperlink{classlbann_1_1smooth__relu__layer}{lbann\+::smooth\+\_\+relu\+\_\+layer}\hypertarget{layers_expLinUn}{}\subsection{Exponential Linear Unit}\label{layers_expLinUn}
Exponential linear unit.

Tries to speed up learning by pushing the mean of activations more towards zero by allowing negative values. Helps avoid the need for batch normalization. See\+: Djork-\/\+Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter "Fast and Accurate Deep Network Learning by Exponential Linear Units (E\+L\+Us)" I\+C\+LR 2016.

Implementation details\+: \hyperlink{classlbann_1_1elu__layer}{lbann\+::elu\+\_\+layer}\hypertarget{layers_seluLayer}{}\subsection{Scaled Elu}\label{layers_seluLayer}
S\+E\+LU\+: scaled exponential linear unit. See\+: Klambauer et al. \char`\"{}\+Self-\/\+Normalizing Neural Networks\char`\"{}, 2017. \href{https://arxiv.org/abs/1706.02515}{\tt https\+://arxiv.\+org/abs/1706.\+02515} By default, this assumes the goal is to normalize to 0 mean/unit variance. To accomplish this, you should also normalize input to 0 mean/unit variance (z-\/score), initialize with 0 mean, 1/n variance (He), and use the S\+E\+LU dropout.

Implementation details\+: \hyperlink{classlbann_1_1selu__layer}{lbann\+::selu\+\_\+layer}\hypertarget{layers_sigLayer}{}\subsection{Sigmoid}\label{layers_sigLayer}
Sigmoid activation function. See \href{https://en.wikipedia.org/wiki/Sigmoid_function}{\tt https\+://en.\+wikipedia.\+org/wiki/\+Sigmoid\+\_\+function}

Implementation details\+: \hyperlink{classlbann_1_1sigmoid__layer}{lbann\+::sigmoid\+\_\+layer}\hypertarget{layers_softplus}{}\subsection{Softplus}\label{layers_softplus}
Softplus activation function. This is a smooth approximation of the Re\+LU. See \href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{\tt https\+://en.\+wikipedia.\+org/wiki/\+Rectifier\+\_\+(neural\+\_\+networks)}

Implementation details\+: \hyperlink{classlbann_1_1softplus__layer}{lbann\+::softplus\+\_\+layer}\hypertarget{layers_softmax}{}\subsection{Softmax}\label{layers_softmax}
Softmax layer.

Implementation details\+: \hyperlink{classlbann_1_1softmax__layer}{lbann\+::softmax\+\_\+layer}\hypertarget{layers_tanh}{}\subsection{Tanh}\label{layers_tanh}
Hyperbolic tangent activation function.

Implementation details\+: \hyperlink{classlbann_1_1tanh__layer}{lbann\+::tanh\+\_\+layer}\hypertarget{layers_atan}{}\subsection{Atan}\label{layers_atan}
Arctangent activation function.

Implementation details\+: \hyperlink{classlbann_1_1atan__layer}{lbann\+::atan\+\_\+layer}\hypertarget{layers_bent_identity}{}\subsection{Bent Identity}\label{layers_bent_identity}
Bent identity activation function. See \href{https://en.wikipedia.org/wiki/Bent_Identity_function}{\tt https\+://en.\+wikipedia.\+org/wiki/\+Bent\+\_\+\+Identity\+\_\+function}

Implementation details\+: \hyperlink{classlbann_1_1bent__identity__layer}{lbann\+::bent\+\_\+identity\+\_\+layer}\hypertarget{layers_exponential}{}\subsection{Exponential}\label{layers_exponential}
Exponential activation function.

Implementation details\+: \hyperlink{classlbann_1_1exponential__layer}{lbann\+::exponential\+\_\+layer}\hypertarget{layers_i_o}{}\section{IO}\label{layers_i_o}
\hypertarget{layers_input}{}\subsection{Input}\label{layers_input}
Implementation details\+: \hyperlink{classlbann_1_1input__layer}{lbann\+::input\+\_\+layer} \hypertarget{layers_target}{}\subsection{Target}\label{layers_target}
Implementation details\+: \hyperlink{classlbann_1_1target__layer}{lbann\+::target\+\_\+layer} 