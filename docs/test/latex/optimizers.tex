\hypertarget{optimizers_Adagrad}{}\section{Adagrad}\label{optimizers_Adagrad}
Ada\+Grad optimizer.

Implementation details\+: \hyperlink{classlbann_1_1adagrad}{lbann\+::adagrad}\hypertarget{optimizers_Adam}{}\section{Adam}\label{optimizers_Adam}
Adam optimizer. Reference\+: Kingma, D. and Ba, J. 2014. Adam\+: A Method for Stochastic Optimization.

Implementation details\+: \hyperlink{classlbann_1_1adam}{lbann\+::adam}\hypertarget{optimizers_hadam}{}\section{Hypergradient Adam}\label{optimizers_hadam}
Hypergradient Adam optimizer. Reference\+: Baydin et al. \char`\"{}\+Online Learning Rate Adaptation with Hypergradient Descent\char`\"{}, 2017.

Implementation details\+: \hyperlink{classlbann_1_1hypergradient__adam}{lbann\+::hypergradient\+\_\+adam}\hypertarget{optimizers_rmsp}{}\section{R\+M\+Sprop}\label{optimizers_rmsp}
R\+M\+Sprop optimizer.

Implementation details\+: \hyperlink{classlbann_1_1rmsprop}{lbann\+::rmsprop}\hypertarget{optimizers_SGD}{}\section{S\+GD}\label{optimizers_SGD}
Stochastic gradient descent optimizer. Supports momentum and Nesterov acceleration.

Implementation details\+: \hyperlink{classlbann_1_1sgd}{lbann\+::sgd} 