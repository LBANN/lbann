<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="layers" kind="page">
    <compoundname>layers</compoundname>
    <title>Layers</title>
    <detaileddescription>
<para>Detailed information for layer types and options within these types can be found in this section. The layers are <ref refid="layers_1learning" kindref="member">Learning</ref>, <ref refid="layers_1regularizer" kindref="member">Regularizer</ref>, <ref refid="layers_1transform" kindref="member">Transform</ref>, <ref refid="layers_1activation" kindref="member">Activation</ref>, and <ref refid="layers_1i_o" kindref="member">IO</ref>.</para><sect1 id="layers_1learning">
<title>Learning</title>
<sect2 id="layers_1conv">
<title>Convolution</title>
<para>Implementation details: <ref refid="classlbann_1_1convolution__layer" kindref="compound">lbann::convolution_layer</ref></para></sect2>
<sect2 id="layers_1deconv">
<title>Deconvolution</title>
<para>Implementation details: <ref refid="classlbann_1_1deconvolution__layer" kindref="compound">lbann::deconvolution_layer</ref></para></sect2>
<sect2 id="layers_1ip">
<title>Fully Connected</title>
<para>Fully-connected layer. This layer applies an affine transformation.</para><para>Implementation details: <ref refid="classlbann_1_1fully__connected__layer" kindref="compound">lbann::fully_connected_layer</ref></para></sect2>
</sect1>
<sect1 id="layers_1regularizer">
<title>Regularizer</title>
<sect2 id="layers_1batchNorm">
<title>Batch Normalization</title>
<para>Batch normalization layer. Each input channel is normalized across the mini-batch to have zero mean and unit standard deviation. Learned scaling factors and biases are then applied. See: Sergey Ioffe and Christian Szegedy. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ICML 2015. This uses the standard approach of maintaining the running mean and standard deviation (with exponential decay) for use at test time. See: <ulink url="https://cthorey.github.io/backpropagation/">https://cthorey.github.io/backpropagation/</ulink></para><para>Implementation details: <ref refid="classlbann_1_1batch__normalization" kindref="compound">lbann::batch_normalization</ref></para></sect2>
<sect2 id="layers_1dropout">
<title>Dropout</title>
<para>Dropout layer: Probabilistically drop layer outputs. See: Srivastava, Nitish, et al. &quot;Dropout: a simple way to prevent
  neural networks from overfitting.&quot; Journal of Machine Learning Research 15.1 (2014). The weights are multiplied by 1/(keep probability) at training time, as discussed in section 10 of the paper. Keep probabilities of 0.5 for fully-connected layers and 0.8 for input layers are good starting points.</para><para>Implementation details: <ref refid="classlbann_1_1dropout" kindref="compound">lbann::dropout</ref></para></sect2>
<sect2 id="layers_1selu_dropout">
<title>Selu Dropout</title>
<para>SELU dropout: alpha-scaled dropout for use with SELU activations. See: Klambauer et al. &quot;Self-Normalizing Neural Networks&quot;, 2017. This makes the same default assumptions as our SELU activations. The paper recommends a default dropout rate of 0.05 (keep 0.95).</para><para>Implementation details: <ref refid="classlbann_1_1selu__dropout" kindref="compound">lbann::selu_dropout</ref></para></sect2>
<sect2 id="layers_1local_response_norm_layer">
<title>Local Response Norm Layer</title>
<para>Local Response Normalization layer.</para><para>Implementation details: <ref refid="classlbann_1_1local__response__normalization__layer" kindref="compound">lbann::local_response_normalization_layer</ref></para></sect2>
</sect1>
<sect1 id="layers_1transform">
<title>Transform</title>
<sect2 id="layers_1concatenation">
<title>Concatenation</title>
<para>Concatenation layer. This layer concatenates input tensors along a specified axis.</para><para>Implementation details: <ref refid="classlbann_1_1concatenation__layer" kindref="compound">lbann::concatenation_layer</ref></para></sect2>
<sect2 id="layers_1noise">
<title>Noise</title>
<para>Implementation details: lbann::noise_layer</para></sect2>
<sect2 id="layers_1unpooling">
<title>Unpooling</title>
<para>Unpooling layer.</para><para>Implementation details: <ref refid="classlbann_1_1unpooling__layer" kindref="compound">lbann::unpooling_layer</ref></para></sect2>
<sect2 id="layers_1pooling">
<title>Pooling</title>
<para>Pooling layer.</para><para>Implementation details: <ref refid="classlbann_1_1pooling__layer" kindref="compound">lbann::pooling_layer</ref></para></sect2>
<sect2 id="layers_1reshape">
<title>Reshape</title>
<para>Reshape layer</para><para>Implementation details: <ref refid="classlbann_1_1pooling__layer" kindref="compound">lbann::pooling_layer</ref></para></sect2>
<sect2 id="layers_1slice">
<title>Slice</title>
<para>Slice layer. This layer slices an input tensor along a specified axis.</para><para>Implementation details: <ref refid="classlbann_1_1slice__layer" kindref="compound">lbann::slice_layer</ref></para></sect2>
<sect2 id="layers_1split">
<title>Split</title>
<para>Split layer. This layer can accommodate an arbitrary number of outputs.</para><para>Implementation details: <ref refid="classlbann_1_1split__layer" kindref="compound">lbann::split_layer</ref></para></sect2>
<sect2 id="layers_1sum">
<title>Sum</title>
<para>Sum layer. This layer performs a weighted sum of input tensors, possibly with a different scaling factor for each input. If the scaling factors are not provided, they are all set to one so that this layer performs a simple sum.</para><para>Implementation details: <ref refid="classlbann_1_1sum__layer" kindref="compound">lbann::sum_layer</ref></para></sect2>
</sect1>
<sect1 id="layers_1activation">
<title>Activation</title>
<sect2 id="layers_1idlayer">
<title>Identity</title>
<para>Implementation details: lbann::id_layer</para></sect2>
<sect2 id="layers_1reluLayer">
<title>Rectified Linear Unit</title>
<para>Rectified linear unit activation function. See <ulink url="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</ulink></para><para>Implementation details: <ref refid="classlbann_1_1relu__layer" kindref="compound">lbann::relu_layer</ref></para></sect2>
<sect2 id="layers_1leakyrelu">
<title>Leaky Relu</title>
<para>Leaky rectified linear unit activation function. This is a ReLU variant that avoids the dying ReLU problem where a ReLU neuron can stop updating. See: Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. &quot;Rectifier
nonlinearities improve neural network acoustic models.&quot; Proc. ICML. Vol. 30. No. 1. 2013.</para><para>Implementation details: <ref refid="classlbann_1_1leaky__relu__layer" kindref="compound">lbann::leaky_relu_layer</ref>:</para></sect2>
<sect2 id="layers_1smoothrelu">
<title>Smooth Relu</title>
<para>Smooth Rectified linear unit activation function. This is an approximation to the softplus.</para><para>Implementation details: <ref refid="classlbann_1_1smooth__relu__layer" kindref="compound">lbann::smooth_relu_layer</ref></para></sect2>
<sect2 id="layers_1expLinUn">
<title>Exponential Linear Unit</title>
<para>Exponential linear unit.</para><para>Tries to speed up learning by pushing the mean of activations more towards zero by allowing negative values. Helps avoid the need for batch normalization. See: Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)" ICLR 2016.</para><para>Implementation details: <ref refid="classlbann_1_1elu__layer" kindref="compound">lbann::elu_layer</ref></para></sect2>
<sect2 id="layers_1seluLayer">
<title>Scaled Elu</title>
<para>SELU: scaled exponential linear unit. See: Klambauer et al. &quot;Self-Normalizing Neural Networks&quot;, 2017. <ulink url="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</ulink> By default, this assumes the goal is to normalize to 0 mean/unit variance. To accomplish this, you should also normalize input to 0 mean/unit variance (z-score), initialize with 0 mean, 1/n variance (He), and use the SELU dropout.</para><para>Implementation details: <ref refid="classlbann_1_1selu__layer" kindref="compound">lbann::selu_layer</ref></para></sect2>
<sect2 id="layers_1sigLayer">
<title>Sigmoid</title>
<para>Sigmoid activation function. See <ulink url="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</ulink></para><para>Implementation details: <ref refid="classlbann_1_1sigmoid__layer" kindref="compound">lbann::sigmoid_layer</ref></para></sect2>
<sect2 id="layers_1softplus">
<title>Softplus</title>
<para>Softplus activation function. This is a smooth approximation of the ReLU. See <ulink url="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</ulink></para><para>Implementation details: <ref refid="classlbann_1_1softplus__layer" kindref="compound">lbann::softplus_layer</ref></para></sect2>
<sect2 id="layers_1softmax">
<title>Softmax</title>
<para>Softmax layer.</para><para>Implementation details: <ref refid="classlbann_1_1softmax__layer" kindref="compound">lbann::softmax_layer</ref></para></sect2>
<sect2 id="layers_1tanh">
<title>Tanh</title>
<para>Hyperbolic tangent activation function.</para><para>Implementation details: <ref refid="classlbann_1_1tanh__layer" kindref="compound">lbann::tanh_layer</ref></para></sect2>
<sect2 id="layers_1atan">
<title>Atan</title>
<para>Arctangent activation function.</para><para>Implementation details: <ref refid="classlbann_1_1atan__layer" kindref="compound">lbann::atan_layer</ref></para></sect2>
<sect2 id="layers_1bent_identity">
<title>Bent Identity</title>
<para>Bent identity activation function. See <ulink url="https://en.wikipedia.org/wiki/Bent_Identity_function">https://en.wikipedia.org/wiki/Bent_Identity_function</ulink></para><para>Implementation details: <ref refid="classlbann_1_1bent__identity__layer" kindref="compound">lbann::bent_identity_layer</ref></para></sect2>
<sect2 id="layers_1exponential">
<title>Exponential</title>
<para>Exponential activation function.</para><para>Implementation details: <ref refid="classlbann_1_1exponential__layer" kindref="compound">lbann::exponential_layer</ref></para></sect2>
</sect1>
<sect1 id="layers_1i_o">
<title>IO</title>
<sect2 id="layers_1input">
<title>Input</title>
<para>Implementation details: <ref refid="classlbann_1_1input__layer" kindref="compound">lbann::input_layer</ref> </para></sect2>
<sect2 id="layers_1target">
<title>Target</title>
<para>Implementation details: <ref refid="classlbann_1_1target__layer" kindref="compound">lbann::target_layer</ref> </para></sect2>
</sect1>
    </detaileddescription>
  </compounddef>
</doxygen>
