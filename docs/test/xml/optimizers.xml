<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="optimizers" kind="page">
    <compoundname>optimizers</compoundname>
    <title>Optimizers</title>
    <detaileddescription>
<sect1 id="optimizers_1Adagrad">
<title>Adagrad</title>
<para>AdaGrad optimizer.</para><para>Implementation details: <ref refid="classlbann_1_1adagrad" kindref="compound">lbann::adagrad</ref></para></sect1>
<sect1 id="optimizers_1Adam">
<title>Adam</title>
<para>Adam optimizer. Reference: Kingma, D. and Ba, J. 2014. Adam: A Method for Stochastic Optimization.</para><para>Implementation details: <ref refid="classlbann_1_1adam" kindref="compound">lbann::adam</ref></para></sect1>
<sect1 id="optimizers_1hadam">
<title>Hypergradient Adam</title>
<para>Hypergradient Adam optimizer. Reference: Baydin et al. &quot;Online Learning Rate Adaptation with Hypergradient Descent&quot;, 2017.</para><para>Implementation details: <ref refid="classlbann_1_1hypergradient__adam" kindref="compound">lbann::hypergradient_adam</ref></para></sect1>
<sect1 id="optimizers_1rmsp">
<title>RMSprop</title>
<para>RMSprop optimizer.</para><para>Implementation details: <ref refid="classlbann_1_1rmsprop" kindref="compound">lbann::rmsprop</ref></para></sect1>
<sect1 id="optimizers_1SGD">
<title>SGD</title>
<para>Stochastic gradient descent optimizer. Supports momentum and Nesterov acceleration.</para><para>Implementation details: <ref refid="classlbann_1_1sgd" kindref="compound">lbann::sgd</ref> </para></sect1>
    </detaileddescription>
  </compounddef>
</doxygen>
