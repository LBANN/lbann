# LBANN implementation of MNIST VAE in Doersch's autoencoder tutorial
# See https://github.com/cdoersch/vae_tutorial/blob/master/mnist_vae.prototxt
model {
  name: "directed_acyclic_graph_model"
  data_layout: "data_parallel"
  mini_batch_size: 100 
  block_size: 256
  num_epochs: 50
  num_parallel_readers: 0
  procs_per_model: 0
  num_gpus: -1

  ###################################################
  # Objective function
  ###################################################

  objective_function {
    binary_cross_entropy {}
    layer_term {
      scale_factor: 1.0
      layer: "klloss"
    }
    l2_weight_regularization {
      scale_factor: 0.0005
    }
  }

  ###################################################
  # Metrics
  ###################################################

  metric { mean_squared_error {} }

  ###################################################
  # Callbacks
  ###################################################
  callback {
    print {
      interval: 1
    }
  }
  callback { timer {} }
  callback {
    dump_activations {
      basename: "dump_acts/"
      layer_names: "relu1 sum"
    }
  }
  callback {
    save_images {
      image_dir: "images_"
      extension: "png"
    }
  }

  ###################################################
  # start of layers
  ###################################################

  ######################
  # Data
  ######################
  layer {
    name: "data"
    data_layout: "data_parallel"
    input {
      io_buffer: "partitioned"
    }
  }

  ######################
  # Encoder
  ######################

  layer {
    parents: "data"
    name: "encode1"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 1000
      has_bias: true
    }
  }
  layer {
    parents: "encode1"
    name: "encode1neuron"
    data_layout: "data_parallel"
    relu {}
  }
  layer {
    parents: "encode1neuron"
    name: "encode2"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 500
      has_bias: true
    }
  }
  layer {
    parents: "encode2"
    name: "encode2neuron"
    data_layout: "data_parallel"
    relu {}
  }
  layer {
    parents: "encode2neuron"
    name: "encode3"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 250
      has_bias: true
    }
  }
  layer {
    parents: "encode3"
    name: "encode3neuron"
    data_layout: "data_parallel"
    relu {}
  }

  ######################
  # Latent space
  ######################

  layer {
    parents: "encode3neuron"
    name: "mu"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 30
      has_bias: true
    }
  }
  layer {
    parents: "encode3"
    name: "logsd"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 30
      has_bias: true
    }
  }

  ######################
  # KL divergence
  ######################

  layer {
    parents: "logsd"
    name: "sd"
    data_layout: "data_parallel"
    exponential {}
  }
  layer {
    parents: "sd"
    name: "var"
    data_layout: "data_parallel"
    power {
      exponent: 2
    }
  }
  layer {
    parents: "mu"
    name: "meansq"
    data_layout: "data_parallel"
    power {
      exponent: 2
    }
  }
  layer {
    parents: "meansq var logsd"
    name: "kldiv_plus_half"
    data_layout: "data_parallel"
    sum {
      scaling_factors: "0.5 0.5 -1"
    }
  }
  layer {
    parents: "kldiv_plus_half"
    name: "kldiv_full"
    data_layout: "data_parallel"
    power {
      exponent: -0.5
    }
  }
  layer {
    parents: "kldiv_full"
    name: "kldiv"
    data_layout: "data_parallel"
    reduction {
      mode: "sum"
    }    
  }
  layer {
    parents: "kldiv"
    name: "klloss"
    data_layout: "data_parallel"
    evaluation {}    
  }

  ######################
  # Generate sample  
  ######################

  layer {
    name: "noise"
    data_layout: "data_parallel"
    gaussian {
      neuron_dims: "30"
      mean: 0
      stdev: 1
    }
  }
  layer {
    parents: "noise sd"
    name: "sdnoise"
    data_layout: "data_parallel"
    hadamard {}
  }
  layer {
    parents: "mu sdnoise"
    name: "sample"
    data_layout: "data_parallel"
    sum {}
  }

  ######################
  # Decoder
  ######################

  layer {
    parents: "sample"
    name: "decode4"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 250
      has_bias: true
    }
  }
  layer {
    parents: "decode4"
    name: "decode4neuron"
    data_layout: "data_parallel"
    relu {}
  }
  layer {
    parents: "decode4neuron"
    name: "decode3"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 500
      has_bias: true
    }
  }
  layer {
    parents: "decode3"
    name: "decode3neuron"
    data_layout: "data_parallel"
    relu {}
  }
  layer {
    parents: "decode3neuron"
    name: "decode2"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 1000
      has_bias: true
    }
  }
  layer {
    parents: "decode2"
    name: "decode2neuron"
    data_layout: "data_parallel"
    relu {}
  }
  layer {
    parents: "decode2neuron"
    name: "decode1"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 784
      has_bias: true
    }
  }

  ######################
  # Reconstruction error
  ######################

  layer {
    parents: "decode1"
    name: "loss_sigmoid"
    data_layout: "data_parallel"
    sigmoid {}
  }
  layer {
    parents: "loss_sigmoid"
    name: "reconstruction"
    data_layout: "data_parallel"
    reconstruction {
      original_layer: "data"
    }
  }

  ###################################################
  # end of layers
  ###################################################
}
