# cmd line for original experiment:
#  $ lbann --model=models/autoencoder_mnist/model_autoencoder_mnist.prototext --reader=data_readers/data_reader_mnist.prototext --optimizer=optimizers/opt_sgd.prototext 
#
# Experiment conducted at: Tue Jul 23 09:08:16 2019
#
#
# Experiment was run with lbann version: 0.99.0
#
#
# To rerun the experiment: 
#  $ srun -n1 lbann --prototext=model.prototext
#
#
# Selected SLURM Environment Variables:
# SLURM_NODELIST=pascal17
# SLURM_NNODES=1
# SLURM_TASKS_PER_NODE=36

#
#
data_reader {
  reader {
    name: "mnist"
    role: "train"
    shuffle: true
    data_filedir: "/p/lscratchh/brainusr/datasets/MNIST"
    data_filename: "train-images-idx3-ubyte"
    label_filename: "train-labels-idx1-ubyte"
    validation_percent: 0.1
    percent_of_data_to_use: 1
    transforms {
      scale {
        scale: 0.00392156886
      }
    }
  }
  reader {
    name: "mnist"
    role: "test"
    shuffle: true
    data_filedir: "/p/lscratchh/brainusr/datasets/MNIST"
    data_filename: "t10k-images-idx3-ubyte"
    label_filename: "t10k-labels-idx1-ubyte"
    percent_of_data_to_use: 1
    transforms {
      scale {
        scale: 0.00392156886
      }
    }
  }
}
model {
  objective_function {
    layer_term {
      layer: "mean_squared_error"
    }
  }
  num_epochs: 10
  data_layout: "model_parallel"
  layer {
    input {
    }
    name: "data"
    data_layout: "data_parallel"
    children: "image dummy"
  }
  layer {
    name: "image"
    data_layout: "data_parallel"
    parents: "data"
    split {
    }
  }
  layer {
    name: "dummy"
    data_layout: "data_parallel"
    parents: "data"
    dummy {
    }
  }
  layer {
    fully_connected {
      num_neurons: 1000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "encode1"
    data_layout: "model_parallel"
    parents: "image"
  }
  layer {
    name: "relu1"
    data_layout: "model_parallel"
    parents: "encode1"
    relu {
    }
  }
  layer {
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "encode2"
    data_layout: "model_parallel"
    parents: "relu1"
  }
  layer {
    name: "relu2"
    data_layout: "model_parallel"
    parents: "encode2"
    relu {
    }
  }
  layer {
    fully_connected {
      num_neurons: 250
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "encode3"
    data_layout: "model_parallel"
    parents: "relu2"
  }
  layer {
    name: "relu3"
    data_layout: "model_parallel"
    parents: "encode3"
    relu {
    }
  }
  layer {
    fully_connected {
      num_neurons: 30
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "encode4"
    data_layout: "model_parallel"
    parents: "relu3"
  }
  layer {
    fully_connected {
      num_neurons: 250
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "decode4"
    data_layout: "model_parallel"
    parents: "encode4"
  }
  layer {
    name: "relu4"
    data_layout: "model_parallel"
    parents: "decode4"
    relu {
    }
  }
  layer {
    fully_connected {
      num_neurons: 500
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "decode3"
    data_layout: "model_parallel"
    parents: "relu4"
  }
  layer {
    name: "relu5"
    data_layout: "model_parallel"
    parents: "decode3"
    relu {
    }
  }
  layer {
    fully_connected {
      num_neurons: 1000
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "decode2"
    data_layout: "model_parallel"
    parents: "relu5"
  }
  layer {
    name: "relu6"
    data_layout: "model_parallel"
    parents: "decode2"
    relu {
    }
  }
  layer {
    fully_connected {
      weight_initialization: "glorot_uniform"
      has_bias: true
    }
    name: "decode1"
    data_layout: "model_parallel"
    hint_layer: "image"
    parents: "relu6"
  }
  layer {
    name: "reconstruction"
    data_layout: "model_parallel"
    parents: "decode1"
    sigmoid {
    }
  }
  layer {
    name: "mean_squared_error"
    data_layout: "data_parallel"
    mean_squared_error {
    }
    parents: "reconstruction image"
  }
  mini_batch_size: 10
  callback {
    print {
      interval: 1
    }
  }
  callback {
    summarize_autoencoder_images {
      reconstruction_layer: "reconstruction"
      image_layer: "image"
      interval: 10
    }
  }
  block_size: 256
  num_parallel_readers: 1
}
optimizer {
  sgd {
    learn_rate: 0.01
    momentum: 0.9
  }
}
